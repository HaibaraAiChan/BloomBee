To utilize all GPUs on a single compute node for inference with tensor parallelism, use a command like the following:

```
python3 flex_opt.py --model facebook/opt-1.3b --gen-len 32
```
